{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2StPehwLMat"
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKIX8PqcLMaw"
   },
   "source": [
    "# Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6DNHHXfLMax"
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EP3O4xptLMay"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oByQ9wXHLMay"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubOR3hWGLMaz"
   },
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IySSjlizLMaz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "075EQI1bLMa0"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O8Ht5u8kLMa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYmxND_xLMa2"
   },
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UI0Xy0gHLMa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xBsUSCGGLMa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m9sP3nzlLMa4"
   },
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qz7xz9dbLMa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.624026  , 0.03586074, 0.62110758])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "      <td>0.686724</td>\n",
       "      <td>-0.087949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009739</td>\n",
       "      <td>0.505706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1</td>\n",
       "      <td>0.793616</td>\n",
       "      <td>1.363207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>1.007401</td>\n",
       "      <td>0.835514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>0.544201</td>\n",
       "      <td>0.703591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bias  experience    salary\n",
       "0       1    0.187893 -1.143335\n",
       "1       1    1.185555  0.043974\n",
       "2       1   -0.310938 -0.351795\n",
       "3       1   -1.629277 -1.341220\n",
       "4       1   -1.308600  0.043974\n",
       "..    ...         ...       ...\n",
       "145     1    0.686724 -0.087949\n",
       "146     1    0.009739  0.505706\n",
       "147     1    0.793616  1.363207\n",
       "148     1    1.007401  0.835514\n",
       "149     1    0.544201  0.703591\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (3,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, parameters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2DsTfXuLMa5"
   },
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2y05lS6xLMa5"
   },
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i] * parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< X와 그에 대응하는 parameter값의 내적을 구현하는 함수이다. 반복문을 통해 1부터 i번째까지의 X와 parameter값을 곱해서 더해준값을 return 해준다. > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOGPEhtOLMa5"
   },
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "- 선형 회귀 분석이란 독립변수들이 변화해 갈 때 종속 변수가 어떻게 변화하는지 알아보는것이 목적.\n",
    "- 2가지의 결과 (참,거짓)으로 종속변수(y)를 갖기위한 함수 식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2awM57u5LMa5"
   },
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X,parameters)\n",
    "    p = 1 / (1 + np.exp(-z)) \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WVaZEwrdLMa5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7646995006417049"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< 로지스틱 함수 또는 시그모이드 함수라고 불리우며 항상 0과 1사이에 값을 가집니다. >**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6cXHl8bLMa6"
   },
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) = -\\frac{1}{2}\\Sigma[y_i\\log(\\theta^{T}X_i)+(1-y_i)\\log(1-\\theta^{T}X_i)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FnGRAur3LMa6"
   },
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = (y*np.log(p) + (1-y)*np.log(1-p))\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< cross entropy함수는 로지스틱 회귀의 목적함수 또는 손실함수입니다. 손실함수란 모델이 나타내는 확률 분포 ( 예측값 ) 와 실제 확률 분포 ( 실제값 )의 차이를 나타내는 함수입니다. Cross-entropy 함수는 실제값(y)에 일어날 확률(p)를 log변환 해준것의 곱과 예측값(y-hat, 여기서는 이중분류라서 1-y를 사용)과 일어날 확률 (q)를 log변환해준것의 곱을 더하여 음수로 만든것이다. >**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C922eXYyLMa6"
   },
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters.T)\n",
    "    loss = ((y - y_hat)**2) / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< MSE란, Mean Squared Error로 실제값(y)에서 예측값(y-hat)을 빼준 값을 제곱한 다음 표본의 개수만큼 나눈것이다 >**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0j-MhGkyLMa6"
   },
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X,y, parameters)\n",
    "    loss = loss / n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uSkPS5olLMa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1368803624228452"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< Data의 손실을 구하고 Data의 batch의 수 만큼 나누어 loss의 평균을 구해주는 함수다(?)  >**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACLi9vCyLMa7"
   },
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caMA-f00LMa7"
   },
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)= -\\Sigma(y_i - \\theta^{T}X_i)X_{ij}$ \n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -\\Sigma(y_i - p_i)X_{ij}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "apZ0Miz5LMa7"
   },
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters.T)\n",
    "        gradient = (y-y_hat) * X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = (y-p) * X[j]\n",
    "    return -gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14826851856802198"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XXBe6q8gLMa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08407694111542571"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< 위 함수는 model이 선형일때와 로지스틱일때를 구분하여 gradient를 구하는 함수이다. 먼저 선형일 때 gradient는 예측값(y_hat)을 X와 parameter를 transpose한 값을 dot_product해주어 구하고 실제값인 y에서 y_hat을 빼준 뒤 X의 j번째의 값과 곱해준것이다.\n",
    "model이 로지스틱일때 기울기(gradient)는 실제값(y)에서 로지스틱함수(p)를 빼주고 X의 j번째와 곱한값이다.  >**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTfzKh_nLMa7"
   },
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.iloc[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Qby2_X1vLMa7"
   },
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(len(X_set)):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52.15389821061275, 97.983568384196, 129.72120573873747]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients2 = batch_gradient(X_train, y_train, parameters, 'linear')\n",
    "gradients2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rHxBS5RnLMa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52.662079106152795, 5.817436762204624, 37.46871865544021]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< 위에 정의된 함수는 batch 하나의 gradient를 구해주는 함수이다. >**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQnlDboALMa8"
   },
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "LgnfT6eHLMa8"
   },
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S9fk1UTLMa8"
   },
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "#### 설명: < index를 활용하여 batch를 부분집합으로 나누는 함수이다. X_train의 크기가 200이고 batch_size가 20이라고 가정할 때 mini batch의 사이즈는 20으로 10개로 나누어진다. batch_size가 커지면 한번 학습할 때 데이터의 크기가 커지고 mini-batch의 개수가 적어지게 되고 batch_size가 작아지면 한번 학습할 때 데이터의 크기는 작아지지만 mini-batch의 개수는 많아진다. >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pMuZbkQLMa8"
   },
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "loeL51rPLMa8"
   },
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= learning_rate * (1/n)\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NLB2dUVTLMa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62418185, 0.44805624, 0.43574939])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< gradient descent의 공식을 보면 alpha(learning_rate)와 n(batch_size)과 gradient((y-p) * X[j])를 곱해준다. learning rate가 클수록 gradient 그래프에서 한번에 많이 움직이게 되고 편미분항이 이동할 방향과 크기를 결정하게된다. >**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX8RJFd_LMa9"
   },
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 한번 학습을 완료한 상태의 횟수\n",
    "- num_epoch: 총 횟수\n",
    "<br\n",
    "\n",
    "BGD: \"배치 경사 하강법 - 매 step에서 train data 전체를 대상으로 gradient의 평균을 구하여 최적의 해를 찾는 기법\"\n",
    "\n",
    "SGD: \"loss function을 계산할 때, 일부 데이터의 모음(Mini-Batch)를 사용하여 loss function을 계산하는 기법, 다소 정확도는 BGD에 비해 낮을수도 있지만 계산 속도가 훨씬 빨\n",
    " 서 더 많은 step을 나아갈 수 있고 local minima에 빠질 가능성이 적다.\"\n",
    "\n",
    "MGD: \"전체 train data를 batch_size로 등분하여 batch set을 순차적으로 수행, BGD보다 빠르고 SGD보다 낮은 오차율을 보여준다.\"\n",
    "<br\n",
    "\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요\n",
    "\n",
    "batch_size=1 -> \"SGD\" * (SGD는 하나의 batch를 보고 gradient를 바로 계산하여 적용한다)\n",
    "\n",
    "batch_size=k -> \"MGD\"\n",
    "\n",
    "batch_size=whole -> \"BGD\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZGbnVHbbLMa9"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, model, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, batch_size)\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, batch_size)\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CTtc3eiLMa9"
   },
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnUpYC7_LMa9"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-LS6o3aeLMa-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9407442704541807  params: [0.36424034 0.16457728 0.80028429]  gradients: [0.02985160067971767, 0.004157745195278829, 0.028821510596084857]\n",
      "epoch: 100  loss: 0.47961606233886445  params: [-0.81745526  0.68216404 -0.59567898]  gradients: [0.003463701844948239, -0.007050985502951244, 0.007528620361523285]\n",
      "epoch: 200  loss: 0.40408127515386  params: [-1.01451293  1.27354563 -1.19175426]  gradients: [0.0012564161659728522, -0.0049204829347018406, 0.004837862045726561]\n",
      "epoch: 300  loss: 0.368158205516972  params: [-1.11879682  1.69417298 -1.60295184]  gradients: [0.0009031414214740496, -0.0036192516492136863, 0.0035208665160749494]\n",
      "epoch: 400  loss: 0.34771868379988924  params: [-1.20057916  2.01277551 -1.91179677]  gradients: [0.0007454583643815325, -0.0028191927037198214, 0.0027233202959414653]\n",
      "epoch: 500  loss: 0.3348479721259819  params: [-1.26913098  2.26604984 -2.1557589 ]  gradients: [0.000631498232509359, -0.002284135963908753, 0.002193962166473218]\n",
      "epoch: 600  loss: 0.3261791846375774  params: [-1.32756321  2.47419906 -2.35521661]  gradients: [0.000541389857191037, -0.0019023779809046331, 0.0018186938778930583]\n",
      "epoch: 700  loss: 0.32005558980006005  params: [-1.37791142  2.64936599 -2.52234253]  gradients: [0.00046884832370745034, -0.001616629790406504, 0.0015393859757553622]\n",
      "epoch: 800  loss: 0.31557425884499374  params: [-1.42171464  2.79939118 -2.66495415]  gradients: [0.00040976602696407165, -0.0013948793114146433, 0.001323719553211428]\n",
      "epoch: 900  loss: 0.31220440423619794  params: [-1.46015564  2.92962823 -2.78836342]  gradients: [0.00036105611136365523, -0.0012179215848819765, 0.001152390914884625]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.49382871,  3.0428233 , -2.89533021])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = X_train.shape[0])\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9227137430300998  params: [0.32062099 0.76764838 0.620417  ]  gradients: [0.1462672594257117, 0.0691410957356273, 0.17055752897257437]\n",
      "epoch: 100  loss: 0.33141184902629967  params: [-1.29073547  2.34404167 -2.23061358]  gradients: [0.003012097978809764, -0.010756877226719213, 0.010314830175947209]\n",
      "epoch: 200  loss: 0.30881321249216026  params: [-1.50568895  3.08253554 -2.93279537]  gradients: [0.0015428436233057272, -0.005159241392792489, 0.004865096837933593]\n",
      "epoch: 300  loss: 0.30251687825574636  params: [-1.62496899  3.4779381  -3.30400692]  gradients: [0.0009228300542421634, -0.0030370495611848633, 0.0028390287386713836]\n",
      "epoch: 400  loss: 0.3001554295396636  params: [-1.69924488  3.72151313 -3.53110243]  gradients: [0.0005965347409430432, -0.0019499787983228395, 0.0018134176453566736]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.72570691,  3.80792238, -3.61138655])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd2 = gradient_descent(X_train, y_train, learning_rate = 0.5, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = X_train.shape[0])\n",
    "new_param_bgd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "x0H5tnauLMa-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.2665249981539225  params: [-0.90727116  1.23280668 -1.34977627]  gradients: [0.02418152932985493, 0.01315961139016817, 0.01701390015879907]\n",
      "epoch: 100  loss: 0.07736664557061775  params: [-1.9303264   4.17502159 -4.06769304]  gradients: [0.0075385368602801, 0.004102478969734366, 0.005304044741532554]\n",
      "epoch: 200  loss: 0.07736266551585466  params: [-1.9303681   4.1751431  -4.06780374]  gradients: [0.0075381595212657805, 0.004102273621481862, 0.005303779249295563]\n",
      "epoch: 300  loss: 0.0773626651836168  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489767101, 0.00410227360434025, 0.005303779227133383]\n",
      "epoch: 400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.9303681 ,  4.17514311, -4.06780375])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.1013456312190334  params: [-1.29682021  2.37345296 -3.17362468]  gradients: [0.05243761263299083, 0.028536598949776695, 0.036894618770120576]\n",
      "epoch: 100  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n",
      "epoch: 200  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n",
      "epoch: 300  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n",
      "epoch: 400  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n",
      "epoch: 500  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n",
      "epoch: 600  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n",
      "epoch: 700  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n",
      "epoch: 800  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n",
      "epoch: 900  loss: 0.04527745616703725  params: [-2.10636691  4.22380215 -4.6397031 ]  gradients: [0.023022802186761605, 0.01252903097061125, 0.016198630468658323]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.10636691,  4.22380215, -4.6397031 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd2 = gradient_descent(X_train, y_train, learning_rate = 0.5, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 1)\n",
    "new_param_sgd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iGfXGoJaLMa-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.46326514213928616  params: [0.50525509 0.16929855 0.60479829]  gradients: [0.07076640800730387, 0.009114722677107917, 0.07087610866896263]\n",
      "epoch: 100  loss: 0.17541671992888871  params: [-1.40293996  2.74294502 -2.61528865]  gradients: [-0.005785440459194763, 0.0007154887462567028, 0.01099147375780448]\n",
      "epoch: 200  loss: 0.16750022298661776  params: [-1.61852252  3.46170493 -3.29284231]  gradients: [-0.004299438655209298, 0.0031274073189200497, 0.008367872127761456]\n",
      "epoch: 300  loss: 0.1657763296262523  params: [-1.72223543  3.80052994 -3.60858218]  gradients: [-0.0036567896648431963, 0.004013073414219187, 0.007456868552214768]\n",
      "epoch: 400  loss: 0.16526226516100945  params: [-1.77917296  3.9854013  -3.7799242 ]  gradients: [-0.003322969122525738, 0.004442871862654655, 0.007026071614923202]\n",
      "epoch: 500  loss: 0.16508687332307054  params: [-1.81233122  4.09275762 -3.87913255]  gradients: [-0.0031347139518594427, 0.0046770342351734745, 0.006794592978445153]\n",
      "epoch: 600  loss: 0.16502241806327078  params: [-1.83224341  4.15712884 -3.9385179 ]  gradients: [-0.003023824685817247, 0.004812338766878817, 0.006661893781951001]\n",
      "epoch: 700  loss: 0.1649975263767999  params: [-1.84440958  4.19642451 -3.97473346]  gradients: [-0.002956867106025573, 0.004893124926802973, 0.006583034268829449]\n",
      "epoch: 800  loss: 0.1649875153593146  params: [-1.85191892  4.22066634 -3.99706149]  gradients: [-0.002915838655442018, 0.004942292952347825, 0.006535175191320071]\n",
      "epoch: 900  loss: 0.16498332442429472  params: [-1.85658242  4.23571637 -4.01091816]  gradients: [-0.002890473884770198, 0.004972564253104118, 0.006505761198092829]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.85946666,  4.24502253, -4.01948441])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.5, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 100)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.648239646588072  params: [-0.19426456  0.63376576 -0.01644145]  gradients: [0.06741034821212942, 0.02583640486468207, 0.12497356158372182]\n",
      "epoch: 100  loss: 0.3343129464776427  params: [-1.62773889  3.49088986 -3.33002791]  gradients: [-0.008723586216737545, 0.0064776510353849485, 0.016697025094877065]\n",
      "epoch: 200  loss: 0.33026165259188534  params: [-1.78256557  3.99297034 -3.79627067]  gradients: [-0.00680404792883913, 0.009002787570273556, 0.01415509421229803]\n",
      "epoch: 300  loss: 0.3298274726010855  params: [-1.83400773  4.1586803  -3.94915561]  gradients: [-0.006212604487433361, 0.009719304716277338, 0.013453975794865534]\n",
      "epoch: 400  loss: 0.329766282655861  params: [-1.85310489  4.22008435 -4.00568662]  gradients: [-0.005998799579097311, 0.009971948776147073, 0.01320897871096581]\n",
      "epoch: 500  loss: 0.32975648703554056  params: [-1.8604539   4.24369859 -4.02740971]  gradients: [-0.005917346662676183, 0.01006733805666431, 0.013116780186137687]\n",
      "epoch: 600  loss: 0.32975470208673585  params: [-1.86331953  4.25290434 -4.03587565]  gradients: [-0.005885708965089879, 0.010104262499013401, 0.013081135660984325]\n",
      "epoch: 700  loss: 0.32975431181344006  params: [-1.86444261  4.25651184 -4.03919285]  gradients: [-0.005873328662798607, 0.010118692426011435, 0.013067212722989392]\n",
      "epoch: 800  loss: 0.32975420605505995  params: [-1.86488363  4.2579284  -4.04049536]  gradients: [-0.00586847001795236, 0.010124352506293522, 0.013061752551310741]\n",
      "epoch: 900  loss: 0.32975417181687616  params: [-1.86505694  4.25848508 -4.04100721]  gradients: [-0.005866561080462069, 0.01012657587081095, 0.013059607874879088]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.86512466,  4.25870258, -4.0412072 ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd2 = gradient_descent(X_train, y_train, learning_rate = 0.5, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 50)\n",
    "new_param_mgd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< loss값은 SGD기법과 learning_rate를 올린것이 제일 작았다. >**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0oCaZ0tLMa-"
   },
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BGD를 이용해 얻은 parameter로 예측 \n",
    "y_predict_bgd = []\n",
    "y_predict_proba_bgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    y_predict_proba_bgd.append(p)\n",
    "    if p > 0.5:\n",
    "        y_predict_bgd.append(1)\n",
    "    else:\n",
    "        y_predict_bgd.append(0)\n",
    "\n",
    "### SGD를 이용해 얻은 parameter로 예측         \n",
    "y_predict_sgd = []\n",
    "y_predict_proba_sgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_sgd)\n",
    "    y_predict_proba_sgd.append(p)\n",
    "    if p > 0.5:\n",
    "        y_predict_sgd.append(1)\n",
    "    else:\n",
    "        y_predict_sgd.append(0)\n",
    "\n",
    "### MGD를 이용해 얻은 parameter로 예측 \n",
    "y_predict_mgd = []\n",
    "y_predict_proba_mgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_mgd)\n",
    "    y_predict_proba_mgd.append(p)\n",
    "    if p > 0.5:\n",
    "        y_predict_mgd.append(1)\n",
    "    else:\n",
    "        y_predict_mgd.append(0)\n",
    "\n",
    "\n",
    "### random_parameter로 예측         \n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZKpFItfLMa-"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "W4E1PgX5LMa-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "def get_clf_eval(y_test, y_pred):\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print('confusion matrix')\n",
    "    print(confusion)\n",
    "    print('precision : {0:.4f}, accuracy : {1:.4f}, recall : {2:.4f}'.format(precision, accuracy, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[38  2]\n",
      " [ 4  6]]\n",
      "precision : 0.7500, accuracy : 0.8800, recall : 0.6000\n"
     ]
    }
   ],
   "source": [
    "get_clf_eval(y_test, y_predict_bgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[38  2]\n",
      " [ 1  9]]\n",
      "precision : 0.8182, accuracy : 0.9400, recall : 0.9000\n"
     ]
    }
   ],
   "source": [
    "get_clf_eval(y_test, y_predict_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[38  2]\n",
      " [ 1  9]]\n",
      "precision : 0.8182, accuracy : 0.9400, recall : 0.9000\n"
     ]
    }
   ],
   "source": [
    "get_clf_eval(y_test, y_predict_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[ 8 32]\n",
      " [ 1  9]]\n",
      "precision : 0.2195, accuracy : 0.3400, recall : 0.9000\n"
     ]
    }
   ],
   "source": [
    "get_clf_eval(y_test, y_predict_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(y, predict_proba):\n",
    "    score = roc_auc_score(y, predict_proba)\n",
    "    return print('ROC_AUC : {0:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC : 0.9525\n"
     ]
    }
   ],
   "source": [
    "roc_auc(y_test, y_predict_proba_bgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC : 0.9525\n"
     ]
    }
   ],
   "source": [
    "roc_auc(y_test, y_predict_proba_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC : 0.9500\n"
     ]
    }
   ],
   "source": [
    "roc_auc(y_test, y_predict_proba_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1score(y_test, y_pred):\n",
    "    return print('F1-score : {0:.4f}'.format(f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score : 0.6667\n"
     ]
    }
   ],
   "source": [
    "f1score(y_test, y_predict_bgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score : 0.8571\n"
     ]
    }
   ],
   "source": [
    "f1score(y_test, y_predict_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score : 0.8571\n"
     ]
    }
   ],
   "source": [
    "f1score(y_test, y_predict_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score : 0.3529\n"
     ]
    }
   ],
   "source": [
    "f1score(y_test, y_predict_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< MGD와 SGD와 왜 같게 나오는지는 정확히 모르지만 점수가 다 똑같이 나왔고 BGD는 SGD와 MGD보다 점수가 낮게 나왔다. 그리고 roc_auc의 점수가 제일 높게 나왔다. >**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIgqa85aLMa_"
   },
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYeIg9QNLMa_"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "nv8-yhszLMa_"
   },
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "07XtxLGWLMa_"
   },
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oENC02TLMa_"
   },
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "fu578YrKLMa_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44428813, 2.8805832 ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "M74iqj4WLMa_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.48868168402311923  params: [0.88144598 1.2514324 ]  gradients: [0.0064784734374964265, 0.0037493006474613403]\n",
      "epoch: 100  loss: 0.6352784432214742  params: [0.40176683 2.85319931]  gradients: [0.030743372000371208, 0.026644547895773415]\n",
      "epoch: 200  loss: 0.63553822731808  params: [0.40018664 2.8560316 ]  gradients: [0.030757235003213364, 0.026665245440741406]\n",
      "epoch: 300  loss: 0.6355387426513447  params: [0.4001835  2.85603722]  gradients: [0.030757262484082837, 0.02666528646984155]\n",
      "epoch: 400  loss: 0.6355387436729005  params: [0.4001835  2.85603723]  gradients: [0.030757262538558677, 0.026665286551174283]\n",
      "epoch: 500  loss: 0.6355387436749254  params: [0.4001835  2.85603723]  gradients: [0.030757262538666664, 0.0266652865513355]\n",
      "epoch: 600  loss: 0.6355387436749294  params: [0.4001835  2.85603723]  gradients: [0.030757262538666865, 0.026665286551335807]\n",
      "epoch: 700  loss: 0.6355387436749294  params: [0.4001835  2.85603723]  gradients: [0.030757262538666865, 0.026665286551335807]\n",
      "epoch: 800  loss: 0.6355387436749294  params: [0.4001835  2.85603723]  gradients: [0.030757262538666865, 0.026665286551335807]\n",
      "epoch: 900  loss: 0.6355387436749294  params: [0.4001835  2.85603723]  gradients: [0.030757262538666865, 0.026665286551335807]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.4001835 , 2.85603723])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X,y, model = 'linear')\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Ii3zBOwSLMa_"
   },
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCVynFSPLMbA"
   },
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "UoEACrbYLMbA"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< 선에 가깝게 분포되고 있다. >**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "wk3_optimization_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
